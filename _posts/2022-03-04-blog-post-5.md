---
layout: post
title: Blog Post 5
---

In this blog post, we will work with Tensorflow and explore `Dataset`s, data augmentation, and transfer learning as we try to construct a model that can effectively distinguish images of cats from images of dogs.

## Load Packages and Obtain Data

As always, we begin by importing the packages which we anticipate we will need. In this case, those packages include `pyplot`, `numpy`, `os`, and a number of `tensorflow` packages.


```python
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import datasets, layers, models, utils
```

We now access the data, prvoided by the TensorFlow team, which contains labeled images of cats and dogs by running the following code block. We will acess the data in the form of `Dataset`s, which will allow us to access data in small batches; this is useful with it's not pratical to load all the data into memory. In this case, data will be access in batches of 32 at a time, each with the dimensions 132 by 132. 


```python
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    

Let's take a look at our dataset! To do so, we will write a function that creates a two-row visualization, in which three random pictures of cats are displayed in the top row and three random pictures of dogs in the bottom row. We will use the `take` method (e.g. `train_dataset.take(1)`) in order to retrieve a single batch of 32 labeled images from the training data. 


```python
def two_row_visualization():
  # Retrieves the class names in the dataset. 
  class_names = train_dataset.class_names

  plt.figure(figsize=(15, 10))

  # For each of the images and labels in a batch of training data: 
  for images, labels in train_dataset.take(1):
    # cat_pos and dog_pos mark where in the plot 
    # we want to insert our next cat/dog photo. 
    cat_pos = 1
    dog_pos = 4

    # For each of the 32 labels:
    for i in range(len(labels)):
      # If we encounter a cat picture:
      if (class_names[labels[i]] == "cats" and cat_pos < 4):
        ax = plt.subplot(2, 3, cat_pos)
        cat_pos += 1
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
      # If we encounter a dog picture:
      elif (class_names[labels[i]] == "dogs" and dog_pos < 7):
        ax = plt.subplot(2, 3, dog_pos)
        dog_pos += 1
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
      # If we have filled up our plot with 3 cat pics 
      # and 3 dog pics, we exit the function
      if (cat_pos == 4 and dog_pos == 7):
        return
```


```python
two_row_visualization()
```


    
![output_6_0.png](/images/output_6_0.png)
    


Aww, cute :') Let's also add some technical code that will help us more rapidly read data in the future! We saved this step until after the two-row visualization because trying to put it before resulted in an error when trying to retrieve the `class_names` of the training dataset. 


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Finally, we want to check the label frequencies. First, we add a line of code that creates an iterator called `labels`. We then use this iterator to sum up the values of the labels of the dataset. We know that we have a total of 2000 images (this was told to us when we loaded in the dataset), and we know that labels of `0` correspond to cats and `1` to dogs. Thus, the sum will tell us how many dogs we have in the set, and we can subtract this number from 2000 to get the number of cats (since each image in the dataset is labeled either "dogs" or "cats"). 




```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
sum(labels_iterator)
```




    1000



This means we must have 1000 images labeled "dogs", and the remaining 2000 - 1000 = 1000 images must be of "cats". Our dataset set is split evenly, with 50% of the images being cats and the remaining 50% being dogs. We thus expect that our baseline machine learning model would guess accurately 50% of the time. 

## First Model

We begin by creating a super simple model, which consists of two `Cov2D` layers, two `MaxPooling2D` layers, one `Flatten` layer, one `Dense` layer, and one `Dropout` layer (with an input parameter of 0.5). We then build upon this model and see how we can manipulate its structure to maximize our accuracy. We tried the following: changing the order of the layers in our model; the input for `Dropout`; and adding more `Cov2D`, `MaxPooling2D`, `Dropout`, and `Dense` layers. Ultimately, however, we found that the model displayed below (a variation of our initial simple model, with a `Dropout` input of 0.3) was the best model (although not by much--for all of the models we tried, the accuracy score hovered in the 55% to 60% range). 



```python
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (160, 160, 3)), 
    layers.MaxPooling2D((2, 2)), 
    layers.Conv2D(32, (3, 3), activation = 'relu'), 
    layers.MaxPooling2D((2, 2)), 
    layers.Dropout(0.3),
    layers.Flatten(), 
    layers.Dense(10)                                                
])
```


```python
# We compile our model...
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

#... and fit it to our training dataset.
history = model1.fit(train_dataset,  
                    epochs=20, 
                    validation_data=validation_dataset)
```


```python
# We plot our history to visualize the accuracy 
# over time of the training and validation sets. 
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fad1a490b10>




    
![output_15_1.png](/images/output_15_1.png)
    


As desired, we have achieve a validation accuracy a bit above our baseline, at **around 55% to 60%**. This is some improvement, but not that much better than the baseline. We do not seem to observe much overfitting, and the accuracy seems to be more or less constant, rather than increasing. There is a bit of a downward trend for the validation set's accuracy around the 16th epoch, but this is paired with a downward trend in the accuracy of the training set as well, which suggests that this is not a case of overfitting. 

## Model with Data Augmentation

Now we're going to spice up our model by adding some data augmentation layers! This refers to the method of evaluating modified copies of images in the training set, and this helps our model learn about *invariant* features of our inputs. For instance, we want our model to recognize a cat as a cat even if its photo has been flipped or rotated. 

We begin by creating `RandomFlip()` and `RandomRotation()` layers, applying these layers to images, and then plotting the results alongside the original image. 


```python
# Our flip layer:
flip = tf.keras.Sequential([
  tf.keras.layers.RandomFlip()
])

# Our rotation layer:
rotate = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.25)               
])
```


```python
# For each image in our dataset batch:
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(15, 10))
  first_image = image[0]
  
  # We want 6 images: 3 that display the 
  # flips, and 3 that display the rotations.
  for i in range(6):
    # Create a 2 by 3 plot (i.e., we will show
    # 2 demonstrations of each transformation). 
    ax = plt.subplot(2, 3, i + 1)
    # The original image should be displayed 
    # at the beginning of each row.
    if i == 0 or i == 3:
      original_image = tf.expand_dims(first_image, 0)
      plt.imshow(original_image[0] / 255)
      plt.axis('off')
    # Flipped images go in the top row...
    elif i < 3:
      flipped_image = flip(tf.expand_dims(first_image, 0))
      plt.imshow(flipped_image[0] / 255)
      plt.axis('off')
    # and rotated images in the bottom row.
    else:
      rotated_image = rotate(tf.expand_dims(first_image, 0))
      plt.imshow(rotated_image[0] / 255)
      plt.axis('off')
      
```


    
![output_19_0.png](/images/output_19_0.png)
    


Lovely! Now let's create a model out of it. We will use the same model we did in the previous part, but with our `flip` and `rotate` layers applied as the first two layers!


```python
model2 = models.Sequential([
    # Apply the transformations to 
    # the beginning of the model. 
    layers.RandomFlip(),
    layers.RandomRotation(0.25),                        
    layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (160, 160, 3)), 
    layers.MaxPooling2D((2, 2)), 
    layers.Conv2D(32, (3, 3), activation = 'relu'), 
    layers.MaxPooling2D((2, 2)), 
    layers.Dropout(0.3),
    layers.Flatten(), 
    layers.Dense(10)                                                
])
```


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model2.fit(train_dataset,  
                    epochs=20, 
                    validation_data=validation_dataset)
```


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7efc0610dd90>




    
![output_23_1.png](/images/output_23_1.png)
    


The validation accuracy for this model appears to be **between 55% and 62%**. Our results for this model are fairly similar to the results we obtained for `model1`, although the accuracy in this case seems to fluctuate more drastically than it did for `model1`. We observe more overfitting this time; our model reaches an accuracy of about 62% around the 17th epoch, but this accuracy then sharply falls, even while the training set's accuracy continues to increase. This is likely the result of overfitting. 

## Data Preprocessing

We will now make simple transformations to our input data prior to the training process, which will allow our training energy to be spent on handling the actual data signal instead of cleaning up the data. 

The following code block creates a preprocessing layer, called `preprocessor`, which we will include as the first line in our model.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model3 = models.Sequential([
    # Apply the preprocessor to
    # the beginning of the model. 
    preprocessor,
    layers.RandomFlip(),
    layers.RandomRotation(0.25),                        
    layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (160, 160, 3)), 
    layers.MaxPooling2D((2, 2)), 
    layers.Conv2D(32, (3, 3), activation = 'relu'), 
    layers.MaxPooling2D((2, 2)), 
    layers.Dropout(0.3),
    layers.Flatten(), 
    layers.Dense(10)                                                
])
```


```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model3.fit(train_dataset,  
                    epochs=20, 
                    validation_data=validation_dataset)
```


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fad0016c2d0>




    
![output_29_1.png](/images/output_29_1.png)
    


We find that the model stabilizes with an accuracy of **around 70% to 72.5%**. This is a fair bit better than our original model! We also notice that the pattern by which the accuracy of the validation data changes more closely aligns with the accuracy of the training data, which is unlike what happened with `model1`, where the accuracy of the validation data stayed mostly constant, even as the training data's accuracy improved. This time, we do not seem to observe as much overfitting as before; we have inconsistent peaks and dips, but the accuracy of the validation data appears to be on a slight rise near the later epochs. 

## Transfer Learning

Our final model: transfer learning! To do this, we will first access a pre-existing "base model", which has already been trained on a related task, and incorporate this base model into our own model. 

We begin by downloading `MobileNetV2`, which we will save as `base_model` and incorporate into our model. 


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model4 = models.Sequential([
    # Preprocessing layer: 
    preprocessor,
    # Transformation layers: 
    layers.RandomFlip(),
    layers.RandomRotation(0.25),
    # Base model layer:  
    base_model_layer,
    # Need to flatten for the dimensions to fit:
    layers.Flatten(), 
    # Dense layer with input 2: 
    layers.Dense(2)                                 
])
```

Notice how this model is much simpler than our previous models, without any `Conv2D`, `MaxPooling2D`, or `Dropout` layers. We can see why when we call `model4.summary()`: 


```python
model4.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_1 (RandomFlip)  (None, 160, 160, 3)       0         
                                                                     
     random_rotation_1 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     flatten_3 (Flatten)         (None, 32000)             0         
                                                                     
     dense_3 (Dense)             (None, 2)                 64002     
                                                                     
    =================================================================
    Total params: 2,321,986
    Trainable params: 64,002
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

Over 2 million total parameters, roughly 64 thousand of which are trainable! This is evidently a very complex model, even without the extra layers. Now, let's actually run this model and see the results.


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model4.fit(train_dataset,  
                    epochs=20, 
                    validation_data=validation_dataset)
```


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7facfc2f5750>




    
![output_38_1.png](/images/output_38_1.png)
    


This time, our validation accuracy stabilizes pretty early on, **around 96%**. This is much, *much* better than the accuracies we achieved with any of the previous models. And would ya look at that--with this model, the accuracy of the validation data was consistently higher than the accuracy of the training data, which is a sign that no overfitting occurred with `model4`. Nice! 

## Score on Test Data

Since our `model4` was the best model by far, we will score our unseen `test_dataset` on this model. 


```python
model4.evaluate(test_dataset)
```

    6/6 [==============================] - 0s 37ms/step - loss: 0.3768 - accuracy: 0.9688

    [0.3767501413822174, 0.96875]



We achieve an accuracy of **96.875%**! Not too shabby, considering our baseline of 50%; we've improved our model by a whoppin' 93.75%! :D 

And that concludes Blog Post 6! Thanks for tuning in ʕ •ᴥ•ʔ
