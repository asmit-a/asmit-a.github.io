---
layout: post
title: Blog Post 6
---

In this blog post we will use Tensorflow to develop and assess a fake news classifier. 

Our data for this assignment comes from the following article: 

> Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138)

which can be accessed [here on Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset).





## 1. Acquire Training Data



```python
from matplotlib import pyplot as plt

import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, losses, models, utils

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

import re
import string

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
```

We first download the data from the given url, and convert it to a pandas `DataFrame`. 


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df
```





  <div id="df-d65f4836-2c09-4c4f-94a0-5cdcdb78d34f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-d65f4836-2c09-4c4f-94a0-5cdcdb78d34f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-d65f4836-2c09-4c4f-94a0-5cdcdb78d34f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-d65f4836-2c09-4c4f-94a0-5cdcdb78d34f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




We see that each row contains three pieces of information concerning one article: the article's title, its text, and whether it contains fake news (indicated by a 1 in the "fake" column) or not (indicated by a 0 in the "fake" column). 

Our data looks good to go! Now, let's turn it into a `Dataset` so that we can apply start using machine learning on it! 

## 2. Make a Dataset

In this section, our goal is to transform the data frame that we currently have into a dataset that we can use to build our Tensorflow models. A good approach is to write a `make_dataset()` function that can take in a data frame of articles (in the format we saw above) and return a dataset version of it. 

Let's also add some extra functionality to `make_dataset()`, and use it to remove stopwords (i.e. common and uninformative words) from the article texts and titles.  




```python
def make_dataset(article_df):

  # remove stopwords
  stop = stopwords.words('english')
  article_df['title_sans_sw'] = article_df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  article_df['text_sans_sw'] = article_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  article_data = tf.data.Dataset.from_tensor_slices(
      (
          {
              # predictors
              "title" : df[["title_sans_sw"]],
              "text"  : df[["text_sans_sw"]]
          },
          {
              # target
              "fake" : df[["fake"]]
          }
      )
  )

  article_data = article_data.batch(100)

  return article_data

```

Now that we've created our `make_dataset` function, we can pass our article data frame into it and receive a dataset, which we shall call just `data`, in return. 


```python
data = make_dataset(df)
data
```




    <BatchDataset element_spec=({'title': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'text': TensorSpec(shape=(None, 1), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)})>




```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8 * len(data))

train = data.take(train_size)
val = data.skip(train_size)
```


```python
len(data), len(train), len(val), len(data) == len(train) + len(val)
```




    (225, 180, 45, True)




```python
# proportion of fake articles in original data frame
df["fake"].sum() / len(df["fake"])
```




    0.522963160942581



We can expect the base rate to be **approximately 52.3%**.


```python
def standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
  
  return no_punctuation
```


```python
size_vocabulary = 3000

vectorize_layer = TextVectorization(
    standardize = standardization, # calling our standardization function
    max_tokens = size_vocabulary,  # only consider this many words
    output_mode='int'
) 

vectorize_layer.adapt(train.map(lambda x, y: x['title']))
vectorize_layer.adapt(train.map(lambda x, y: x['text']))
```

## 3. Create Models

Now that we have our dataset, it's time to move on to the fun part: modeling! We want to use TensorFlow models to examine the following question:


> *When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?*

In order to adequately explore this question, we will create three TensorFlow models: 

1. A model which takes in only the article title as input
2. A model which takes in only the article text as input
3. A model which takes in both the article ttitle and text as inputs

For these models, we'll be using the Functional API rather than the Sequential API. This will allow us to more flexibly and effectively deal with the way each of our articles has two input categories (text and title). Before we begin building our models, let's set up our inputs so that we can easily call them when the time comes. 


```python
title_input = keras.Input (
    shape = (1,), # each title is a single string
    name = "title", 
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text", 
    dtype = "string"
)
```

As per recommendation, we'll also be sharing an embedding layer for both the article `title` and `text` inputs. Therefore, let us define our embedding layer here, before we get into the nitty-gritty of model building. We save this embedding layer as `shared_embedding` so that we can easily call it in the future. 


```python
shared_embedding = layers.Embedding(size_vocabulary, 32)
```

### 3.1. First Model: Title-only

It's finally time to get modeling! Our first model will take in only the article title as input. We've already defined this input above, storing it as `title_input`; now it's just a matter of taking this input and putting it through layer after layer, saving each output in the variable `title_features` as we go. 

Of course, there remains the question of how exactly we want to build our model. Our first step is easy: we want to call `vectorize_layer` on our `title_input`, so we can standardize our text and convert it to the sweet, sweet numbers that our program can actually consume and interpret. But what do we do after that? 

Looking back at previous lecture notes and referencing the text classification models we built there, we opt to include our `shared_embedding` layer first, and then to add a couple of `Dropout` layers and a `GlobalAveragePooling1D` layer before bringing it all together with a `Dense(32)` layer. 


```python
title_features = vectorize_layer(title_input)
title_features = shared_embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation = 'relu')(title_features)
```

We want our output to have the same number of dimensions as the number of classifications. In the case of these articles, each article either is fake or isn't; thus, we have 2 classifications, and thus our output, which we'll call `title_output`, shall have 2 dimensions. 


```python
title_output = layers.Dense(2, name = "fake")(title_features)
```


```python
title_model = keras.Model(
    inputs = title_input,
    outputs = title_output
)
```

Now that we've created our `title_model`, let's take a look at its structure. 


```python
title_model.summary()
```

    Model: "model_6"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, None)             0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, None, 32)          96000     
                                                                     
     dropout_8 (Dropout)         (None, None, 32)          0         
                                                                     
     global_average_pooling1d_4   (None, 32)               0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_9 (Dropout)         (None, 32)                0         
                                                                     
     dense_6 (Dense)             (None, 32)                1056      
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 97,122
    Trainable params: 97,122
    Non-trainable params: 0
    _________________________________________________________________
    

It's pretty complex, with over 97 thousand parameters, all of which are trainable. We can anticipate a fairly accurate model, it seems.

We compile the model and then fit it to our training data, saving its accuracy on the remaining validation data at each step of the fitting. 


```python
title_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

title_history = title_model.fit(train, 
                                validation_data=val,
                                epochs = 20, 
                                verbose = True)
```

    Epoch 1/20
    

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    

    180/180 [==============================] - 2s 6ms/step - loss: 0.4510 - accuracy: 0.8284 - val_loss: 0.1893 - val_accuracy: 0.9391
    Epoch 2/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.1405 - accuracy: 0.9526 - val_loss: 0.1002 - val_accuracy: 0.9689
    Epoch 3/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0884 - accuracy: 0.9724 - val_loss: 0.0713 - val_accuracy: 0.9760
    Epoch 4/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0705 - accuracy: 0.9771 - val_loss: 0.0479 - val_accuracy: 0.9861
    Epoch 5/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0577 - accuracy: 0.9813 - val_loss: 0.0465 - val_accuracy: 0.9867
    Epoch 6/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0507 - accuracy: 0.9826 - val_loss: 0.0423 - val_accuracy: 0.9871
    Epoch 7/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0441 - accuracy: 0.9862 - val_loss: 0.0373 - val_accuracy: 0.9880
    Epoch 8/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0399 - accuracy: 0.9867 - val_loss: 0.0409 - val_accuracy: 0.9860
    Epoch 9/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0361 - accuracy: 0.9887 - val_loss: 0.0220 - val_accuracy: 0.9951
    Epoch 10/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0350 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9898
    Epoch 11/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0315 - accuracy: 0.9903 - val_loss: 0.0234 - val_accuracy: 0.9926
    Epoch 12/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0295 - accuracy: 0.9901 - val_loss: 0.0261 - val_accuracy: 0.9926
    Epoch 13/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0279 - accuracy: 0.9911 - val_loss: 0.0267 - val_accuracy: 0.9917
    Epoch 14/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.0232 - val_accuracy: 0.9942
    Epoch 15/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.0206 - val_accuracy: 0.9944
    Epoch 16/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0228 - accuracy: 0.9922 - val_loss: 0.0205 - val_accuracy: 0.9926
    Epoch 17/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.0191 - val_accuracy: 0.9940
    Epoch 18/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0179 - val_accuracy: 0.9960
    Epoch 19/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0159 - val_accuracy: 0.9962
    Epoch 20/20
    180/180 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.9936 - val_loss: 0.0195 - val_accuracy: 0.9933
    


```python
plt.plot(title_history.history["accuracy"], label = "training")
plt.plot(title_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa884639750>




    
![output_30_1.png](/images/output_30_1.png)
    


We find that, by the time the validation accuracy stabilizes, it hovers **between 99.2% and 99.6%**. This is already a fantastic model! 

We also notice that the validation accuracy tends to be higher than the training accuracy, which is the opposite of the overfitting that we feared and somewhat unusual. This is likely the result of our use of the `Dropout` function, which dampens the effectiveness of the model on the training set but not the validation set. 

### 3.2. Second Model: Text-only

In our second model, we do the exact same thing that we did in the previous step, but this time use only the text associated with each article rather than the title. 


```python
text_features = vectorize_layer(text_input)
text_features = shared_embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation = 'relu')(text_features)
```


```python
text_output = layers.Dense(2, name = "fake")(text_features)
```


```python
text_model = keras.Model(
    inputs = text_input,
    outputs = text_output
)
```


```python
text_model.summary()
```

    Model: "model_7"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, None)             0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, None, 32)          96000     
                                                                     
     dropout_10 (Dropout)        (None, None, 32)          0         
                                                                     
     global_average_pooling1d_5   (None, 32)               0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_11 (Dropout)        (None, 32)                0         
                                                                     
     dense_7 (Dense)             (None, 32)                1056      
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 97,122
    Trainable params: 97,122
    Non-trainable params: 0
    _________________________________________________________________
    


```python
text_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

text_history = text_model.fit(train, 
                                validation_data=val,
                                epochs = 20, 
                                verbose = True)
```

    Epoch 1/20
    

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    

    180/180 [==============================] - 3s 15ms/step - loss: 0.0329 - accuracy: 0.9900 - val_loss: 0.0241 - val_accuracy: 0.9938
    Epoch 2/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.0202 - val_accuracy: 0.9969
    Epoch 3/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0276 - accuracy: 0.9929 - val_loss: 0.0155 - val_accuracy: 0.9969
    Epoch 4/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.0205 - val_accuracy: 0.9960
    Epoch 5/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.0185 - val_accuracy: 0.9962
    Epoch 6/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0202 - val_accuracy: 0.9969
    Epoch 7/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0101 - val_accuracy: 0.9984
    Epoch 8/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0193 - val_accuracy: 0.9982
    Epoch 9/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 0.0158 - val_accuracy: 0.9957
    Epoch 10/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.0093 - val_accuracy: 0.9980
    Epoch 11/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.0120 - val_accuracy: 0.9955
    Epoch 12/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.0066 - val_accuracy: 0.9991
    Epoch 13/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0127 - accuracy: 0.9968 - val_loss: 0.0134 - val_accuracy: 0.9987
    Epoch 14/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.0101 - val_accuracy: 0.9980
    Epoch 15/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0191 - accuracy: 0.9925 - val_loss: 0.0061 - val_accuracy: 0.9996
    Epoch 16/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.0077 - val_accuracy: 0.9982
    Epoch 17/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0055 - val_accuracy: 0.9991
    Epoch 18/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0072 - val_accuracy: 0.9989
    Epoch 19/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0455 - accuracy: 0.9862 - val_loss: 0.0066 - val_accuracy: 0.9982
    Epoch 20/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0107 - accuracy: 0.9973 - val_loss: 0.0048 - val_accuracy: 0.9991
    


```python
plt.plot(text_history.history["accuracy"], label = "training")
plt.plot(text_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa8843b06d0>




    
![output_39_12.png](/images/output_39_12.png)
    


This time, our validation accuracy hovers between **99.8 and 99.9%**. An even better model than last time. We again observe that the validation accuracy tends to be higher than the training accuracy; this is likely again the effect of our `Dropout` layers. 

### 3.3. Third Model: Title and Text

In our third and final model, we repeat what we did previously, but this time with both the title and text as inputs. 


```python
main = layers.concatenate([title_features, text_features], axis=1)
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name="fake")(main)
```


```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model.summary()
```

    Model: "model_8"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, None)        0           ['title[0][0]',                  
     ization)                                                         'text[0][0]']                   
                                                                                                      
     embedding (Embedding)          (None, None, 32)     96000       ['text_vectorization[4][0]',     
                                                                      'text_vectorization[5][0]']     
                                                                                                      
     dropout_8 (Dropout)            (None, None, 32)     0           ['embedding[0][0]']              
                                                                                                      
     dropout_10 (Dropout)           (None, None, 32)     0           ['embedding[1][0]']              
                                                                                                      
     global_average_pooling1d_4 (Gl  (None, 32)          0           ['dropout_8[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     global_average_pooling1d_5 (Gl  (None, 32)          0           ['dropout_10[0][0]']             
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_9 (Dropout)            (None, 32)           0           ['global_average_pooling1d_4[0][0
                                                                     ]']                              
                                                                                                      
     dropout_11 (Dropout)           (None, 32)           0           ['global_average_pooling1d_5[0][0
                                                                     ]']                              
                                                                                                      
     dense_6 (Dense)                (None, 32)           1056        ['dropout_9[0][0]']              
                                                                                                      
     dense_7 (Dense)                (None, 32)           1056        ['dropout_11[0][0]']             
                                                                                                      
     concatenate_3 (Concatenate)    (None, 64)           0           ['dense_6[0][0]',                
                                                                      'dense_7[0][0]']                
                                                                                                      
     dense_9 (Dense)                (None, 32)           2080        ['concatenate_3[0][0]']          
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_9[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 100,258
    Trainable params: 100,258
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
keras.utils.plot_model(model)
```




    
![output_45_0.png](/images/output_45_0.png)
    




```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model.fit(train, 
                    validation_data=val,
                    epochs = 20, 
                    verbose = True)
```

    Epoch 1/20
    180/180 [==============================] - 4s 17ms/step - loss: 0.0847 - accuracy: 0.9820 - val_loss: 0.0133 - val_accuracy: 0.9982
    Epoch 2/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.0077 - val_accuracy: 0.9989
    Epoch 3/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0089 - accuracy: 0.9967 - val_loss: 0.0064 - val_accuracy: 0.9989
    Epoch 4/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0206 - val_accuracy: 0.9918
    Epoch 5/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0024 - val_accuracy: 0.9996
    Epoch 6/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.0017 - val_accuracy: 0.9998
    Epoch 7/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0041 - val_accuracy: 0.9987
    Epoch 8/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0055 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9996
    Epoch 9/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 5.2578e-04 - val_accuracy: 1.0000
    Epoch 10/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 3.6629e-04 - val_accuracy: 1.0000
    Epoch 11/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0012 - val_accuracy: 0.9998
    Epoch 12/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0014 - val_accuracy: 0.9998
    Epoch 13/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 5.1179e-04 - val_accuracy: 0.9998
    Epoch 14/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 2.0915e-04 - val_accuracy: 1.0000
    Epoch 15/20
    180/180 [==============================] - 3s 15ms/step - loss: 9.3438e-04 - accuracy: 0.9998 - val_loss: 0.0016 - val_accuracy: 0.9993
    Epoch 16/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 8.7867e-05 - val_accuracy: 1.0000
    Epoch 17/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 6.9892e-04 - val_accuracy: 1.0000
    Epoch 18/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 6.0518e-04 - val_accuracy: 0.9998
    Epoch 19/20
    180/180 [==============================] - 3s 15ms/step - loss: 6.0448e-04 - accuracy: 0.9998 - val_loss: 3.9925e-04 - val_accuracy: 0.9998
    Epoch 20/20
    180/180 [==============================] - 5s 29ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9978
    


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa88367d710>




    
![output_47_12.png](/images/output_47_12.png)
    


This time, our final validation accuracy is **between 99.7% and 100%**! The model seems to perform mostly equally well on both the validation and training data this time, but perhaps slightly better on the validation data, on which the model achieved a perfect score multiple times.

## 4. Model Evaluation

For this part, let's focus only on our best model, the model that combines both titles and text. 

We download the test data, turn it into a `DataFrame`, and once again convert this data frame into a dataset using the `make_dataset()` function that we defined in Part 2. We then take our model and evaluate it on the test dataset. 


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_df = pd.read_csv(train_url)
test_data = make_dataset(test_df)

model.evaluate(test_data)
```

    225/225 [==============================] - 2s 8ms/step - loss: 0.0065 - accuracy: 0.9971
    




    [0.006535245571285486, 0.9970600008964539]



On our test data, we get an accuracy of **99.7%**! Our model performed slightly worse than it did on the validation set in the previous part, but still very very well. 

## 5. Embedding Visualization

Finally, we want to turn our attention back to the embedding layer that we used in our models. Let's explore this layer to see how our model represents words in the vector space; in other words, let's see how words are spatially represented and given distances between each other. 


```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()   
```


```python
weights
```




    array([[-4.1748714e-03, -8.3982310e-04, -3.2772012e-03, ...,
            -6.9777167e-04,  7.3565473e-03,  3.8303584e-03],
           [-8.3988339e-02, -7.9752475e-02, -8.5896604e-02, ...,
            -6.8885684e-02,  4.2912822e-02, -5.3047974e-02],
           [ 9.9109948e-01,  1.0113870e+00,  1.0315191e+00, ...,
             1.0050050e+00, -9.4922227e-01,  9.3795943e-01],
           ...,
           [ 1.3529450e+00,  1.2339970e+00,  1.3596787e+00, ...,
             1.3705751e+00, -1.1494076e+00,  1.4531122e+00],
           [ 2.4570622e-01,  1.9774707e-01,  2.0898494e-01, ...,
             1.7629947e-01, -5.0439939e-02,  1.4754997e-01],
           [ 3.7543485e-01,  4.2985639e-01,  3.3297786e-01, ...,
             4.0283147e-01, -2.0509684e-01,  3.9145365e-01]], dtype=float32)




```python
from sklearn.decomposition import PCA
import plotly.express as px 
```


```python
pca_2 = PCA(n_components=2)
weights_2D = pca_2.fit_transform(weights)
```

We create a 2-dimensional representation, in which each word is assigned a point on the 2D Cartesian coordinate plane. 


```python
embedding_df_2D = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_2D[:,0],
    'x1'   : weights_2D[:,1]
})
embedding_df_2D
```





  <div id="df-10597346-b080-4611-a79b-60858e9b5da3">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.027516</td>
      <td>0.005291</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.384366</td>
      <td>0.844612</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>5.475505</td>
      <td>1.685452</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>-1.299088</td>
      <td>-0.035099</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>2.177100</td>
      <td>2.057247</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2995</th>
      <td>chip</td>
      <td>1.517614</td>
      <td>-0.133069</td>
    </tr>
    <tr>
      <th>2996</th>
      <td>bigotry</td>
      <td>2.481074</td>
      <td>-0.430538</td>
    </tr>
    <tr>
      <th>2997</th>
      <td>shocked</td>
      <td>7.096281</td>
      <td>-0.666456</td>
    </tr>
    <tr>
      <th>2998</th>
      <td>pending</td>
      <td>0.701232</td>
      <td>0.268522</td>
    </tr>
    <tr>
      <th>2999</th>
      <td>highprofile</td>
      <td>1.945989</td>
      <td>-0.267574</td>
    </tr>
  </tbody>
</table>
<p>3000 rows × 3 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-10597346-b080-4611-a79b-60858e9b5da3')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-10597346-b080-4611-a79b-60858e9b5da3 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-10597346-b080-4611-a79b-60858e9b5da3');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




Let us plot these points and see how it looks. 


```python
fig = px.scatter(embedding_df_2D, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df_2D))), 
                 size_max = 5,    
                 hover_name = "word")

fig.show()
```


{% include base_plot.html %}


We see that the words are plotted in a roughly circular shape around the origin. It seems to spread out along the x-axis. Let's see if we can figure out what each section of this shape indicates.


```python
right = embedding_df_2D[embedding_df_2D['x0'] > 5]
right
```





  <div id="df-08003b0f-af54-441b-a197-a55d5084b16d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>5.475505</td>
      <td>1.685452</td>
    </tr>
    <tr>
      <th>77</th>
      <td>including</td>
      <td>13.566589</td>
      <td>-0.592488</td>
    </tr>
    <tr>
      <th>109</th>
      <td>still</td>
      <td>6.982279</td>
      <td>0.992017</td>
    </tr>
    <tr>
      <th>115</th>
      <td>minister</td>
      <td>8.312734</td>
      <td>0.702406</td>
    </tr>
    <tr>
      <th>120</th>
      <td>if</td>
      <td>6.424763</td>
      <td>1.207862</td>
    </tr>
    <tr>
      <th>123</th>
      <td>women</td>
      <td>6.811016</td>
      <td>0.905816</td>
    </tr>
    <tr>
      <th>130</th>
      <td>monday</td>
      <td>5.834674</td>
      <td>0.756711</td>
    </tr>
    <tr>
      <th>250</th>
      <td>border</td>
      <td>5.949790</td>
      <td>0.285800</td>
    </tr>
    <tr>
      <th>519</th>
      <td>talk</td>
      <td>5.896327</td>
      <td>0.626394</td>
    </tr>
    <tr>
      <th>528</th>
      <td>daily</td>
      <td>5.237736</td>
      <td>0.059491</td>
    </tr>
    <tr>
      <th>561</th>
      <td>services</td>
      <td>5.169186</td>
      <td>0.234345</td>
    </tr>
    <tr>
      <th>751</th>
      <td>pressure</td>
      <td>7.061856</td>
      <td>0.312742</td>
    </tr>
    <tr>
      <th>775</th>
      <td>david</td>
      <td>5.012348</td>
      <td>-0.151210</td>
    </tr>
    <tr>
      <th>797</th>
      <td>spent</td>
      <td>7.370992</td>
      <td>-0.118992</td>
    </tr>
    <tr>
      <th>807</th>
      <td>michael</td>
      <td>5.455308</td>
      <td>-0.060636</td>
    </tr>
    <tr>
      <th>848</th>
      <td>hand</td>
      <td>7.352415</td>
      <td>-0.419918</td>
    </tr>
    <tr>
      <th>974</th>
      <td>mainstream</td>
      <td>9.386424</td>
      <td>0.318045</td>
    </tr>
    <tr>
      <th>1011</th>
      <td>development</td>
      <td>5.950166</td>
      <td>-0.655414</td>
    </tr>
    <tr>
      <th>1026</th>
      <td>concerned</td>
      <td>8.026786</td>
      <td>0.968022</td>
    </tr>
    <tr>
      <th>1149</th>
      <td>believes</td>
      <td>7.067733</td>
      <td>-0.060319</td>
    </tr>
    <tr>
      <th>1157</th>
      <td>offer</td>
      <td>5.573434</td>
      <td>0.433011</td>
    </tr>
    <tr>
      <th>1179</th>
      <td>2018</td>
      <td>5.439783</td>
      <td>-0.522962</td>
    </tr>
    <tr>
      <th>1276</th>
      <td>towards</td>
      <td>5.697837</td>
      <td>-0.078651</td>
    </tr>
    <tr>
      <th>1278</th>
      <td>rich</td>
      <td>6.885123</td>
      <td>-0.010789</td>
    </tr>
    <tr>
      <th>1435</th>
      <td>alabama</td>
      <td>10.965442</td>
      <td>0.549635</td>
    </tr>
    <tr>
      <th>1498</th>
      <td>familiar</td>
      <td>5.251266</td>
      <td>-0.141987</td>
    </tr>
    <tr>
      <th>1567</th>
      <td>province</td>
      <td>10.312469</td>
      <td>0.245458</td>
    </tr>
    <tr>
      <th>1629</th>
      <td>requests</td>
      <td>5.575195</td>
      <td>-0.379046</td>
    </tr>
    <tr>
      <th>1637</th>
      <td>aide</td>
      <td>5.098980</td>
      <td>0.091087</td>
    </tr>
    <tr>
      <th>1728</th>
      <td>payments</td>
      <td>5.507584</td>
      <td>-0.413558</td>
    </tr>
    <tr>
      <th>1847</th>
      <td>blamed</td>
      <td>5.781142</td>
      <td>0.513521</td>
    </tr>
    <tr>
      <th>1905</th>
      <td>hes</td>
      <td>6.133975</td>
      <td>0.158453</td>
    </tr>
    <tr>
      <th>1923</th>
      <td>investigate</td>
      <td>9.485009</td>
      <td>0.159793</td>
    </tr>
    <tr>
      <th>2019</th>
      <td>google</td>
      <td>6.527910</td>
      <td>-0.326171</td>
    </tr>
    <tr>
      <th>2088</th>
      <td>struck</td>
      <td>5.966795</td>
      <td>-0.608646</td>
    </tr>
    <tr>
      <th>2110</th>
      <td>poverty</td>
      <td>7.280804</td>
      <td>-0.389891</td>
    </tr>
    <tr>
      <th>2189</th>
      <td>sit</td>
      <td>5.523813</td>
      <td>-0.664719</td>
    </tr>
    <tr>
      <th>2214</th>
      <td>train</td>
      <td>5.881485</td>
      <td>-0.142814</td>
    </tr>
    <tr>
      <th>2314</th>
      <td>talked</td>
      <td>8.334790</td>
      <td>-0.346885</td>
    </tr>
    <tr>
      <th>2340</th>
      <td>effectively</td>
      <td>5.540830</td>
      <td>0.323305</td>
    </tr>
    <tr>
      <th>2400</th>
      <td>bigger</td>
      <td>5.664583</td>
      <td>0.450271</td>
    </tr>
    <tr>
      <th>2428</th>
      <td>reading</td>
      <td>5.046346</td>
      <td>-0.349689</td>
    </tr>
    <tr>
      <th>2473</th>
      <td>miller</td>
      <td>7.791787</td>
      <td>0.186631</td>
    </tr>
    <tr>
      <th>2581</th>
      <td>guess</td>
      <td>5.038390</td>
      <td>0.046796</td>
    </tr>
    <tr>
      <th>2681</th>
      <td>grounds</td>
      <td>7.230068</td>
      <td>0.218266</td>
    </tr>
    <tr>
      <th>2736</th>
      <td>nearby</td>
      <td>5.201006</td>
      <td>-0.080297</td>
    </tr>
    <tr>
      <th>2817</th>
      <td>km</td>
      <td>5.609176</td>
      <td>0.837242</td>
    </tr>
    <tr>
      <th>2829</th>
      <td>feels</td>
      <td>5.975773</td>
      <td>-0.661190</td>
    </tr>
    <tr>
      <th>2836</th>
      <td>opinions</td>
      <td>5.939748</td>
      <td>0.159089</td>
    </tr>
    <tr>
      <th>2844</th>
      <td>minnesota</td>
      <td>5.418726</td>
      <td>-0.198892</td>
    </tr>
    <tr>
      <th>2997</th>
      <td>shocked</td>
      <td>7.096281</td>
      <td>-0.666456</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-08003b0f-af54-441b-a197-a55d5084b16d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-08003b0f-af54-441b-a197-a55d5084b16d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-08003b0f-af54-441b-a197-a55d5084b16d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
left = embedding_df_2D[embedding_df_2D['x0'] < -5]
left
```





  <div id="df-974cb4fc-5826-456c-9f56-39f0989c32b6">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>46</th>
      <td>many</td>
      <td>-5.136037</td>
      <td>0.288528</td>
    </tr>
    <tr>
      <th>222</th>
      <td>something</td>
      <td>-5.540836</td>
      <td>-0.117792</td>
    </tr>
    <tr>
      <th>239</th>
      <td>chief</td>
      <td>-5.336249</td>
      <td>1.199726</td>
    </tr>
    <tr>
      <th>278</th>
      <td>look</td>
      <td>-8.796000</td>
      <td>-0.015823</td>
    </tr>
    <tr>
      <th>412</th>
      <td>ban</td>
      <td>-8.070396</td>
      <td>0.053148</td>
    </tr>
    <tr>
      <th>428</th>
      <td>debate</td>
      <td>-11.313914</td>
      <td>-0.394689</td>
    </tr>
    <tr>
      <th>438</th>
      <td>efforts</td>
      <td>-7.993201</td>
      <td>0.003048</td>
    </tr>
    <tr>
      <th>475</th>
      <td>israel</td>
      <td>-7.078402</td>
      <td>0.177767</td>
    </tr>
    <tr>
      <th>520</th>
      <td>representatives</td>
      <td>-7.934401</td>
      <td>0.394799</td>
    </tr>
    <tr>
      <th>555</th>
      <td>outside</td>
      <td>-5.136232</td>
      <td>0.063591</td>
    </tr>
    <tr>
      <th>596</th>
      <td>list</td>
      <td>-5.518878</td>
      <td>0.565463</td>
    </tr>
    <tr>
      <th>617</th>
      <td>involved</td>
      <td>-5.197159</td>
      <td>0.275529</td>
    </tr>
    <tr>
      <th>660</th>
      <td>denied</td>
      <td>-6.176612</td>
      <td>-0.054119</td>
    </tr>
    <tr>
      <th>662</th>
      <td>calls</td>
      <td>-5.567747</td>
      <td>0.171416</td>
    </tr>
    <tr>
      <th>687</th>
      <td>small</td>
      <td>-8.948973</td>
      <td>-0.360812</td>
    </tr>
    <tr>
      <th>758</th>
      <td>protesters</td>
      <td>-5.704020</td>
      <td>0.051402</td>
    </tr>
    <tr>
      <th>771</th>
      <td>review</td>
      <td>-5.479953</td>
      <td>0.387263</td>
    </tr>
    <tr>
      <th>838</th>
      <td>total</td>
      <td>-7.117056</td>
      <td>-0.104620</td>
    </tr>
    <tr>
      <th>931</th>
      <td>single</td>
      <td>-5.908866</td>
      <td>-0.364929</td>
    </tr>
    <tr>
      <th>1041</th>
      <td>book</td>
      <td>-8.944022</td>
      <td>-1.153508</td>
    </tr>
    <tr>
      <th>1109</th>
      <td>favor</td>
      <td>-5.658415</td>
      <td>1.023216</td>
    </tr>
    <tr>
      <th>1136</th>
      <td>about</td>
      <td>-5.766810</td>
      <td>-0.552869</td>
    </tr>
    <tr>
      <th>1163</th>
      <td>gas</td>
      <td>-5.058475</td>
      <td>-0.267019</td>
    </tr>
    <tr>
      <th>1173</th>
      <td>sought</td>
      <td>-5.769916</td>
      <td>-0.103449</td>
    </tr>
    <tr>
      <th>1198</th>
      <td>my</td>
      <td>-5.467207</td>
      <td>-0.488367</td>
    </tr>
    <tr>
      <th>1243</th>
      <td>w</td>
      <td>-5.643872</td>
      <td>-1.095247</td>
    </tr>
    <tr>
      <th>1268</th>
      <td>canada</td>
      <td>-5.072978</td>
      <td>0.202401</td>
    </tr>
    <tr>
      <th>1352</th>
      <td>begin</td>
      <td>-5.879664</td>
      <td>-0.714402</td>
    </tr>
    <tr>
      <th>1542</th>
      <td>positive</td>
      <td>-5.084573</td>
      <td>-0.579421</td>
    </tr>
    <tr>
      <th>1698</th>
      <td>martin</td>
      <td>-8.716004</td>
      <td>0.280864</td>
    </tr>
    <tr>
      <th>1779</th>
      <td>parenthood</td>
      <td>-6.589599</td>
      <td>0.037256</td>
    </tr>
    <tr>
      <th>1813</th>
      <td>resignation</td>
      <td>-6.577152</td>
      <td>-0.658137</td>
    </tr>
    <tr>
      <th>1864</th>
      <td>natural</td>
      <td>-5.370147</td>
      <td>0.329484</td>
    </tr>
    <tr>
      <th>1890</th>
      <td>players</td>
      <td>-5.941336</td>
      <td>0.694100</td>
    </tr>
    <tr>
      <th>1898</th>
      <td>schumer</td>
      <td>-5.559783</td>
      <td>-0.518472</td>
    </tr>
    <tr>
      <th>1940</th>
      <td>unclear</td>
      <td>-6.347268</td>
      <td>-0.563452</td>
    </tr>
    <tr>
      <th>1959</th>
      <td>st</td>
      <td>-7.097205</td>
      <td>-0.752682</td>
    </tr>
    <tr>
      <th>2074</th>
      <td>orlando</td>
      <td>-7.809639</td>
      <td>-0.042407</td>
    </tr>
    <tr>
      <th>2254</th>
      <td>asian</td>
      <td>-7.089523</td>
      <td>-0.063960</td>
    </tr>
    <tr>
      <th>2285</th>
      <td>condemned</td>
      <td>-5.227015</td>
      <td>0.057078</td>
    </tr>
    <tr>
      <th>2438</th>
      <td>palestinians</td>
      <td>-7.585300</td>
      <td>0.178863</td>
    </tr>
    <tr>
      <th>2538</th>
      <td>ending</td>
      <td>-5.147147</td>
      <td>-0.683803</td>
    </tr>
    <tr>
      <th>2555</th>
      <td>impossible</td>
      <td>-5.168665</td>
      <td>0.188007</td>
    </tr>
    <tr>
      <th>2580</th>
      <td>hosts</td>
      <td>-5.569612</td>
      <td>-0.250291</td>
    </tr>
    <tr>
      <th>2616</th>
      <td>interested</td>
      <td>-6.754365</td>
      <td>-0.550745</td>
    </tr>
    <tr>
      <th>2635</th>
      <td>importance</td>
      <td>-5.038875</td>
      <td>0.382354</td>
    </tr>
    <tr>
      <th>2848</th>
      <td>greens</td>
      <td>-7.538960</td>
      <td>0.004555</td>
    </tr>
    <tr>
      <th>2931</th>
      <td>pursue</td>
      <td>-5.703758</td>
      <td>0.446667</td>
    </tr>
    <tr>
      <th>2944</th>
      <td>fewer</td>
      <td>-5.264465</td>
      <td>-0.329625</td>
    </tr>
    <tr>
      <th>2973</th>
      <td>irs</td>
      <td>-5.359894</td>
      <td>0.312744</td>
    </tr>
    <tr>
      <th>2984</th>
      <td>roll</td>
      <td>-7.697968</td>
      <td>-0.504506</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-974cb4fc-5826-456c-9f56-39f0989c32b6')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-974cb4fc-5826-456c-9f56-39f0989c32b6 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-974cb4fc-5826-456c-9f56-39f0989c32b6');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
top = embedding_df_2D[embedding_df_2D['x1'] > 1]
top
```





  <div id="df-d7f25135-c445-4c84-b707-a2e39c5538f3">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>5.475505</td>
      <td>1.685452</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>2.177100</td>
      <td>2.057247</td>
    </tr>
    <tr>
      <th>8</th>
      <td>president</td>
      <td>1.203436</td>
      <td>1.373692</td>
    </tr>
    <tr>
      <th>10</th>
      <td>it</td>
      <td>-0.684023</td>
      <td>1.510392</td>
    </tr>
    <tr>
      <th>15</th>
      <td>donald</td>
      <td>2.802238</td>
      <td>1.348452</td>
    </tr>
    <tr>
      <th>20</th>
      <td>he</td>
      <td>-1.275303</td>
      <td>1.197031</td>
    </tr>
    <tr>
      <th>25</th>
      <td>united</td>
      <td>0.335239</td>
      <td>1.529632</td>
    </tr>
    <tr>
      <th>26</th>
      <td>in</td>
      <td>1.854105</td>
      <td>1.472711</td>
    </tr>
    <tr>
      <th>38</th>
      <td>this</td>
      <td>-3.937722</td>
      <td>1.027210</td>
    </tr>
    <tr>
      <th>43</th>
      <td>country</td>
      <td>0.717441</td>
      <td>1.050369</td>
    </tr>
    <tr>
      <th>44</th>
      <td>but</td>
      <td>2.995440</td>
      <td>1.696517</td>
    </tr>
    <tr>
      <th>67</th>
      <td>and</td>
      <td>-3.313252</td>
      <td>1.302271</td>
    </tr>
    <tr>
      <th>92</th>
      <td>called</td>
      <td>0.092529</td>
      <td>1.098756</td>
    </tr>
    <tr>
      <th>94</th>
      <td>military</td>
      <td>-0.573526</td>
      <td>1.151192</td>
    </tr>
    <tr>
      <th>117</th>
      <td>there</td>
      <td>-1.446124</td>
      <td>1.013346</td>
    </tr>
    <tr>
      <th>120</th>
      <td>if</td>
      <td>6.424763</td>
      <td>1.207862</td>
    </tr>
    <tr>
      <th>149</th>
      <td>around</td>
      <td>-2.270552</td>
      <td>1.380532</td>
    </tr>
    <tr>
      <th>152</th>
      <td>leader</td>
      <td>1.762263</td>
      <td>1.101700</td>
    </tr>
    <tr>
      <th>234</th>
      <td>came</td>
      <td>4.785649</td>
      <td>1.008311</td>
    </tr>
    <tr>
      <th>239</th>
      <td>chief</td>
      <td>-5.336249</td>
      <td>1.199726</td>
    </tr>
    <tr>
      <th>255</th>
      <td>school</td>
      <td>3.078008</td>
      <td>1.575457</td>
    </tr>
    <tr>
      <th>289</th>
      <td>economic</td>
      <td>0.855299</td>
      <td>1.063221</td>
    </tr>
    <tr>
      <th>317</th>
      <td>stop</td>
      <td>-3.402087</td>
      <td>1.006631</td>
    </tr>
    <tr>
      <th>322</th>
      <td>issues</td>
      <td>0.836817</td>
      <td>1.364815</td>
    </tr>
    <tr>
      <th>328</th>
      <td>talks</td>
      <td>0.085598</td>
      <td>1.008206</td>
    </tr>
    <tr>
      <th>330</th>
      <td>local</td>
      <td>-2.809903</td>
      <td>1.474808</td>
    </tr>
    <tr>
      <th>363</th>
      <td>high</td>
      <td>-0.995758</td>
      <td>1.010060</td>
    </tr>
    <tr>
      <th>422</th>
      <td>gop</td>
      <td>4.647833</td>
      <td>1.161816</td>
    </tr>
    <tr>
      <th>454</th>
      <td>within</td>
      <td>-0.941554</td>
      <td>1.076855</td>
    </tr>
    <tr>
      <th>463</th>
      <td>comes</td>
      <td>0.076508</td>
      <td>1.118745</td>
    </tr>
    <tr>
      <th>474</th>
      <td>getting</td>
      <td>0.210703</td>
      <td>1.257843</td>
    </tr>
    <tr>
      <th>591</th>
      <td>conservatives</td>
      <td>-0.834203</td>
      <td>1.154576</td>
    </tr>
    <tr>
      <th>641</th>
      <td>planned</td>
      <td>-4.142167</td>
      <td>1.220874</td>
    </tr>
    <tr>
      <th>750</th>
      <td>alleged</td>
      <td>-3.299134</td>
      <td>1.432431</td>
    </tr>
    <tr>
      <th>844</th>
      <td>incident</td>
      <td>-1.766655</td>
      <td>1.188725</td>
    </tr>
    <tr>
      <th>1033</th>
      <td>create</td>
      <td>-0.984097</td>
      <td>1.166854</td>
    </tr>
    <tr>
      <th>1084</th>
      <td>french</td>
      <td>3.379338</td>
      <td>1.189312</td>
    </tr>
    <tr>
      <th>1109</th>
      <td>favor</td>
      <td>-5.658415</td>
      <td>1.023216</td>
    </tr>
    <tr>
      <th>1115</th>
      <td>reached</td>
      <td>-1.781570</td>
      <td>1.187709</td>
    </tr>
    <tr>
      <th>1154</th>
      <td>car</td>
      <td>-1.870682</td>
      <td>1.637424</td>
    </tr>
    <tr>
      <th>1252</th>
      <td>higher</td>
      <td>-1.637614</td>
      <td>1.050997</td>
    </tr>
    <tr>
      <th>1338</th>
      <td>steve</td>
      <td>-0.750764</td>
      <td>1.185154</td>
    </tr>
    <tr>
      <th>1373</th>
      <td>briefing</td>
      <td>2.498545</td>
      <td>1.038221</td>
    </tr>
    <tr>
      <th>1379</th>
      <td>helping</td>
      <td>-1.340104</td>
      <td>1.012018</td>
    </tr>
    <tr>
      <th>1671</th>
      <td>study</td>
      <td>-0.313044</td>
      <td>1.045621</td>
    </tr>
    <tr>
      <th>1673</th>
      <td>sarah</td>
      <td>-0.143568</td>
      <td>1.368194</td>
    </tr>
    <tr>
      <th>1911</th>
      <td>venezuela</td>
      <td>-0.421201</td>
      <td>1.270064</td>
    </tr>
    <tr>
      <th>1944</th>
      <td>club</td>
      <td>-1.674363</td>
      <td>1.079274</td>
    </tr>
    <tr>
      <th>2068</th>
      <td>smith</td>
      <td>0.686103</td>
      <td>1.131975</td>
    </tr>
    <tr>
      <th>2152</th>
      <td>reagan</td>
      <td>4.718668</td>
      <td>1.224021</td>
    </tr>
    <tr>
      <th>2160</th>
      <td>mentioned</td>
      <td>-0.607184</td>
      <td>1.200890</td>
    </tr>
    <tr>
      <th>2446</th>
      <td>defending</td>
      <td>2.701318</td>
      <td>1.131872</td>
    </tr>
    <tr>
      <th>2664</th>
      <td>file</td>
      <td>2.960377</td>
      <td>1.115749</td>
    </tr>
    <tr>
      <th>2679</th>
      <td>mrs</td>
      <td>0.210505</td>
      <td>1.081771</td>
    </tr>
    <tr>
      <th>2771</th>
      <td>walked</td>
      <td>4.043686</td>
      <td>1.029709</td>
    </tr>
    <tr>
      <th>2929</th>
      <td>shocking</td>
      <td>-4.162367</td>
      <td>1.078165</td>
    </tr>
    <tr>
      <th>2977</th>
      <td>ballots</td>
      <td>1.032344</td>
      <td>1.071760</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-d7f25135-c445-4c84-b707-a2e39c5538f3')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-d7f25135-c445-4c84-b707-a2e39c5538f3 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-d7f25135-c445-4c84-b707-a2e39c5538f3');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
bottom = embedding_df_2D[embedding_df_2D['x1'] < -0.9]
bottom
```





  <div id="df-deb132bf-cd3d-4152-8461-4ac146dab4d5">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>111</th>
      <td>says</td>
      <td>-0.529581</td>
      <td>-0.928746</td>
    </tr>
    <tr>
      <th>128</th>
      <td>2016</td>
      <td>-2.478393</td>
      <td>-0.947049</td>
    </tr>
    <tr>
      <th>158</th>
      <td>come</td>
      <td>0.686986</td>
      <td>-0.929503</td>
    </tr>
    <tr>
      <th>194</th>
      <td>groups</td>
      <td>-0.278593</td>
      <td>-0.933625</td>
    </tr>
    <tr>
      <th>324</th>
      <td>nations</td>
      <td>1.317629</td>
      <td>-0.993651</td>
    </tr>
    <tr>
      <th>401</th>
      <td>run</td>
      <td>-1.970643</td>
      <td>-0.955635</td>
    </tr>
    <tr>
      <th>470</th>
      <td>economy</td>
      <td>-1.922665</td>
      <td>-0.902285</td>
    </tr>
    <tr>
      <th>599</th>
      <td>try</td>
      <td>-4.575686</td>
      <td>-1.143627</td>
    </tr>
    <tr>
      <th>623</th>
      <td>allies</td>
      <td>1.521149</td>
      <td>-0.980800</td>
    </tr>
    <tr>
      <th>646</th>
      <td>nomination</td>
      <td>2.058765</td>
      <td>-0.922405</td>
    </tr>
    <tr>
      <th>698</th>
      <td>western</td>
      <td>2.295485</td>
      <td>-0.961686</td>
    </tr>
    <tr>
      <th>781</th>
      <td>truth</td>
      <td>-0.316411</td>
      <td>-0.936979</td>
    </tr>
    <tr>
      <th>920</th>
      <td>apparently</td>
      <td>-0.798384</td>
      <td>-0.994958</td>
    </tr>
    <tr>
      <th>970</th>
      <td>happen</td>
      <td>1.557035</td>
      <td>-0.960674</td>
    </tr>
    <tr>
      <th>1008</th>
      <td>why</td>
      <td>-0.895841</td>
      <td>-0.957497</td>
    </tr>
    <tr>
      <th>1041</th>
      <td>book</td>
      <td>-8.944022</td>
      <td>-1.153508</td>
    </tr>
    <tr>
      <th>1124</th>
      <td>impact</td>
      <td>-1.256465</td>
      <td>-0.930534</td>
    </tr>
    <tr>
      <th>1134</th>
      <td>huge</td>
      <td>-1.936409</td>
      <td>-1.174890</td>
    </tr>
    <tr>
      <th>1153</th>
      <td>london</td>
      <td>-1.289271</td>
      <td>-0.901764</td>
    </tr>
    <tr>
      <th>1243</th>
      <td>w</td>
      <td>-5.643872</td>
      <td>-1.095247</td>
    </tr>
    <tr>
      <th>1251</th>
      <td>looks</td>
      <td>0.148677</td>
      <td>-1.122703</td>
    </tr>
    <tr>
      <th>1309</th>
      <td>necessary</td>
      <td>0.130384</td>
      <td>-1.197432</td>
    </tr>
    <tr>
      <th>1356</th>
      <td>communities</td>
      <td>1.820945</td>
      <td>-0.911035</td>
    </tr>
    <tr>
      <th>1464</th>
      <td>conversation</td>
      <td>-4.418407</td>
      <td>-0.951732</td>
    </tr>
    <tr>
      <th>1468</th>
      <td>will</td>
      <td>-1.399826</td>
      <td>-0.920151</td>
    </tr>
    <tr>
      <th>1666</th>
      <td>miles</td>
      <td>-0.984829</td>
      <td>-1.066651</td>
    </tr>
    <tr>
      <th>1713</th>
      <td>jim</td>
      <td>-2.344685</td>
      <td>-1.043049</td>
    </tr>
    <tr>
      <th>1868</th>
      <td>easy</td>
      <td>0.602728</td>
      <td>-0.909979</td>
    </tr>
    <tr>
      <th>2337</th>
      <td>pope</td>
      <td>0.878368</td>
      <td>-1.000374</td>
    </tr>
    <tr>
      <th>2975</th>
      <td>entitled</td>
      <td>0.012968</td>
      <td>-0.968998</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-deb132bf-cd3d-4152-8461-4ac146dab4d5')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-deb132bf-cd3d-4152-8461-4ac146dab4d5 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-deb132bf-cd3d-4152-8461-4ac146dab4d5');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
center = embedding_df_2D[(embedding_df_2D['x1'] < 0.1) & (embedding_df_2D['x1'] > -0.1) & (embedding_df_2D['x0'] > -0.2) & (embedding_df_2D['x0'] < 0.2)]
center
```





  <div id="df-6d96d8d7-fcda-4443-aa8c-97e97d3dbc11">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.027516</td>
      <td>0.005291</td>
    </tr>
    <tr>
      <th>155</th>
      <td>candidate</td>
      <td>0.108024</td>
      <td>-0.052651</td>
    </tr>
    <tr>
      <th>202</th>
      <td>voters</td>
      <td>0.176961</td>
      <td>-0.057944</td>
    </tr>
    <tr>
      <th>252</th>
      <td>control</td>
      <td>0.066808</td>
      <td>-0.002201</td>
    </tr>
    <tr>
      <th>359</th>
      <td>when</td>
      <td>-0.055676</td>
      <td>-0.017788</td>
    </tr>
    <tr>
      <th>465</th>
      <td>lead</td>
      <td>0.011075</td>
      <td>-0.073730</td>
    </tr>
    <tr>
      <th>627</th>
      <td>meet</td>
      <td>0.188105</td>
      <td>-0.067716</td>
    </tr>
    <tr>
      <th>710</th>
      <td>area</td>
      <td>0.154291</td>
      <td>-0.068010</td>
    </tr>
    <tr>
      <th>714</th>
      <td>spoke</td>
      <td>0.171270</td>
      <td>0.034962</td>
    </tr>
    <tr>
      <th>747</th>
      <td>wrong</td>
      <td>0.095332</td>
      <td>-0.010492</td>
    </tr>
    <tr>
      <th>802</th>
      <td>forced</td>
      <td>-0.014404</td>
      <td>0.065524</td>
    </tr>
    <tr>
      <th>1000</th>
      <td>page</td>
      <td>0.187001</td>
      <td>0.008692</td>
    </tr>
    <tr>
      <th>1047</th>
      <td>society</td>
      <td>-0.162226</td>
      <td>-0.098087</td>
    </tr>
    <tr>
      <th>1050</th>
      <td>measure</td>
      <td>0.157214</td>
      <td>-0.019274</td>
    </tr>
    <tr>
      <th>1057</th>
      <td>certain</td>
      <td>-0.123660</td>
      <td>0.087131</td>
    </tr>
    <tr>
      <th>1131</th>
      <td>class</td>
      <td>-0.120756</td>
      <td>-0.078082</td>
    </tr>
    <tr>
      <th>1217</th>
      <td>cover</td>
      <td>-0.033824</td>
      <td>-0.087927</td>
    </tr>
    <tr>
      <th>1275</th>
      <td>arms</td>
      <td>0.043473</td>
      <td>0.029809</td>
    </tr>
    <tr>
      <th>1472</th>
      <td>bit</td>
      <td>0.106616</td>
      <td>0.050339</td>
    </tr>
    <tr>
      <th>1475</th>
      <td>property</td>
      <td>-0.133001</td>
      <td>-0.053996</td>
    </tr>
    <tr>
      <th>1511</th>
      <td>angry</td>
      <td>-0.061637</td>
      <td>0.028103</td>
    </tr>
    <tr>
      <th>1764</th>
      <td>listen</td>
      <td>-0.014293</td>
      <td>-0.037908</td>
    </tr>
    <tr>
      <th>1767</th>
      <td>alternative</td>
      <td>-0.004549</td>
      <td>-0.046857</td>
    </tr>
    <tr>
      <th>1925</th>
      <td>learned</td>
      <td>-0.139950</td>
      <td>-0.037325</td>
    </tr>
    <tr>
      <th>2406</th>
      <td>assistant</td>
      <td>-0.097045</td>
      <td>-0.052079</td>
    </tr>
    <tr>
      <th>2592</th>
      <td>beach</td>
      <td>0.195459</td>
      <td>0.019172</td>
    </tr>
    <tr>
      <th>2595</th>
      <td>nevada</td>
      <td>-0.196786</td>
      <td>0.031798</td>
    </tr>
    <tr>
      <th>2656</th>
      <td>dallas</td>
      <td>0.120971</td>
      <td>0.072118</td>
    </tr>
    <tr>
      <th>2705</th>
      <td>encourage</td>
      <td>0.065781</td>
      <td>0.004479</td>
    </tr>
    <tr>
      <th>2720</th>
      <td>nsa</td>
      <td>0.085480</td>
      <td>0.052848</td>
    </tr>
    <tr>
      <th>2974</th>
      <td>indicated</td>
      <td>0.172832</td>
      <td>-0.014873</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-6d96d8d7-fcda-4443-aa8c-97e97d3dbc11')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-6d96d8d7-fcda-4443-aa8c-97e97d3dbc11 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-6d96d8d7-fcda-4443-aa8c-97e97d3dbc11');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




We first examine the right, left, top, bottom, and center parts of the circle, but it's hard to find a pattern in the words each of these sections contain. All of these words sound political. It seems that perhaps the left contains words related to more controversial topics, such as "ban", "debate", "israel", and "protestors". The top, meanwhile, seems to contain words pertaining to the U.S. government, including "gop", "donald", "president", and "united". 

Since this approach does not shed much light on the arrangement of words, let's try something else. This time, we will split the fake and true articles into two data frames, and examine which words appear most frequently in each category. We will then plot where these particular words are in the scatter plot, and see if this tells us anything. 


```python
fake_df = df[df["fake"]==1]
true_df = df[df["fake"]==0]
```


```python
stop = stopwords.words('english')
```


```python
fake_series = pd.Series(' '.join(fake_df['text']).lower().split()).value_counts()[:100]
fake_words_df = pd.DataFrame({'word':fake_series.index, 'count':fake_series.values})

for word in stop:
  fake_words_df = fake_words_df[fake_words_df["word"] != word]

fake_words_df[:10]
```





  <div id="df-83631652-85bb-4860-b6e1-53d866b70f9b">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14</th>
      <td>trump</td>
      <td>32974</td>
    </tr>
    <tr>
      <th>42</th>
      <td>would</td>
      <td>11686</td>
    </tr>
    <tr>
      <th>45</th>
      <td>people</td>
      <td>11154</td>
    </tr>
    <tr>
      <th>46</th>
      <td>president</td>
      <td>10946</td>
    </tr>
    <tr>
      <th>48</th>
      <td>one</td>
      <td>10720</td>
    </tr>
    <tr>
      <th>50</th>
      <td>said</td>
      <td>10527</td>
    </tr>
    <tr>
      <th>61</th>
      <td>like</td>
      <td>8476</td>
    </tr>
    <tr>
      <th>62</th>
      <td>donald</td>
      <td>8432</td>
    </tr>
    <tr>
      <th>66</th>
      <td>obama</td>
      <td>7807</td>
    </tr>
    <tr>
      <th>68</th>
      <td>clinton</td>
      <td>7499</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-83631652-85bb-4860-b6e1-53d866b70f9b')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-83631652-85bb-4860-b6e1-53d866b70f9b button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-83631652-85bb-4860-b6e1-53d866b70f9b');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
true_series = pd.Series(' '.join(true_df['text']).lower().split()).value_counts()[:100]
true_words_df = pd.DataFrame({'word':true_series.index, 'count':true_series.values})

for word in stop:
  true_words_df = true_words_df[true_words_df["word"] != word]

true_words_df[:10]
```





  <div id="df-292539cb-1210-4b25-a2c9-5c81ef1f083d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>said</td>
      <td>36131</td>
    </tr>
    <tr>
      <th>20</th>
      <td>u.s.</td>
      <td>19046</td>
    </tr>
    <tr>
      <th>22</th>
      <td>trump</td>
      <td>18197</td>
    </tr>
    <tr>
      <th>28</th>
      <td>would</td>
      <td>15881</td>
    </tr>
    <tr>
      <th>34</th>
      <td>president</td>
      <td>11700</td>
    </tr>
    <tr>
      <th>35</th>
      <td>said.</td>
      <td>10782</td>
    </tr>
    <tr>
      <th>47</th>
      <td>new</td>
      <td>8130</td>
    </tr>
    <tr>
      <th>48</th>
      <td>also</td>
      <td>7877</td>
    </tr>
    <tr>
      <th>49</th>
      <td>state</td>
      <td>7853</td>
    </tr>
    <tr>
      <th>50</th>
      <td>government</td>
      <td>7829</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-292539cb-1210-4b25-a2c9-5c81ef1f083d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-292539cb-1210-4b25-a2c9-5c81ef1f083d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-292539cb-1210-4b25-a2c9-5c81ef1f083d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




It appears that the fake articles most frequently contain "trump", and the true articles most frequently contain "said". Notice how there is a lot of overlap; both the true and fake articles contain the opposite's most frequent word in its own list of the top ten most frequent words. The most frequent word in each category, however, is still quite a bit higher in count than any of the subsequent words on the list, so we can still use this most frequent word as an indication of which regions of the plot words that are assoiated with either fake or true articles are most likely to lie in. 


```python
total_words_fake = sum(fake_words_df["count"])
total_words_true = sum(true_words_df["count"])

print(32974 / total_words_fake)
print(36131 / total_words_true)

```

    0.16909917024789997
    0.12500821716851943
    


```python
def mapper(x):
    # the numbers are for the color scale
    if x == "trump":
        return 3
    elif x == "said":
        return 4
    else:
        return 0.5


embedding_df_2D["highlight"] = embedding_df_2D["word"].apply(mapper)
embedding_df_2D["size"]      = np.array((embedding_df_2D["highlight"])**2)
```


```python
fig = px.scatter(embedding_df_2D, 
                 x = "x0", 
                 y = "x1", 
                 size = list(embedding_df_2D["size"]), 
                 color = "highlight",

                 size_max = 20,    
                 hover_name = "word")

fig.show()
```


{% include point_plot.html %}


The word "said" lies in the top left, suggesting that words in the top and the left are most commonly associated with true articles. The word "trump", meanwhile, lies on the opposite end at the bottom right; this suggests that the fourth quadrant contains the words associated with fake articles.

We can draw the conclusion that words on the bottom and right are associated with fake articles, and words on the top and left with true articles.

